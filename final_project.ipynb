{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "# scale to 256 * 256 then perform random crop to 224 * 224\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# scale to 224 * 224\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class ColorizationDataset(datasets.ImageFolder):\n",
    "    '''\n",
    "    Custom Dataset for colorization\n",
    "    '''\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample, target = super(ColorizationDataset, self).__getitem__(index)\n",
    "        # color range L: [0,100], a&b: [-128, 127]\n",
    "        lab = rgb2lab(sample.permute(1, 2, 0))\n",
    "        l = lab[:, :, 0:1]\n",
    "        # normalize a&b to [0, 1]\n",
    "        ab = (lab[:, :, 1:] + 128) / 255\n",
    "        return l, ab, target\n",
    "\n",
    "data_root = '/Users/yuli/Downloads/places365_standard'\n",
    "train_dir = os.path.join(data_root, 'train')\n",
    "#train_set = ColorizationDataset(train_dir, transform=transform_train)\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "val_dir = os.path.join(data_root, 'val')\n",
    "#val_set = ColorizationDataset(val_dir, transform=transform_val)\n",
    "val_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "def classificationLoss(outputs, labels):\n",
    "    '''\n",
    "    Calculate the cross-entropy loss for the classification network.\n",
    "    The classification loss affects the low-level features network,\n",
    "    global features network, and the classification network.\n",
    "    '''\n",
    "    return nn.CrossEntropyLoss()(outputs, labels)\n",
    "\n",
    "\n",
    "def colorizationLoss(coloroutputs, colorlabels, classoutputs, classlabels):\n",
    "    '''\n",
    "    Calculate the loss for the colorization network.\n",
    "    The colorization loss affects the entire network.\n",
    "    '''\n",
    "    # Choose an alpha so that the MSE loss and the cross-entropy loss\n",
    "    # are similar in magnitude.\n",
    "    alpha = 1.0 / 300 \n",
    "    # Calculate the MSE loss for the colorization network\n",
    "    colorLoss = nn.MSELoss()(coloroutputs, colorlabels)\n",
    "    # Calculate the classification loss\n",
    "    classLoss = classificationLoss(classoutputs, classlabels)\n",
    "    return colorLoss - alpha * classLoss\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer, use_gpu):\n",
    "    '''\n",
    "    Train both colorization and classification networks. \n",
    "    '''\n",
    "    model.train()\n",
    "    count = 0\n",
    "    loss_sum = 0.0\n",
    "    for i, (inputs, colors, labels) in enumerate(train_loader):\n",
    "        if use_gpu:\n",
    "            inputs, colors, labels = inputs.cuda(), colors.cuda(), labels.cuda()\n",
    "        # predict and compute loss\n",
    "        color_preds, label_preds = model(inputs)\n",
    "        loss = colorizationLoss(color_preds, colors, label_preds, labels)\n",
    "        \n",
    "        # record loss\n",
    "        loss_sum += loss.item()\n",
    "        count += inputs.size(0)\n",
    "        \n",
    "        # compute gradient and do Adadelta step for both colorization and classification networks.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return loss_sum / count\n",
    "        \n",
    "def validate(val_loader, model, use_gpu):\n",
    "    '''\n",
    "    Evaluate both colorization and classification networks.\n",
    "    '''\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, colors, labels) in enumerate(val_loader):\n",
    "            if use_gpu:\n",
    "                inputs, colors, labels = inputs.cuda(), colors.cuda(), labels.cuda()\n",
    "            # predict and compute losses\n",
    "            color_preds, label_preds = model(inputs)\n",
    "            loss = colorizationLoss(color_preds, colors, label_preds, labels)\n",
    "            \n",
    "            # record loss\n",
    "            loss_sum += loss.item()\n",
    "            count += inputs.size(0)\n",
    "    \n",
    "    return loss_sum / count\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    '''\n",
    "    Save checkpoint for the given state. \n",
    "    '''\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename='checkpoint.pth.tar'):\n",
    "    '''\n",
    "    Load checkpoint from file. \n",
    "    '''\n",
    "    if os.path.isfile(filename):\n",
    "        return torch.load(filename)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Checking GPU availability\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = Net()\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    print('Use GPU')\n",
    "else:\n",
    "    print('Use CPU')\n",
    "# Use ADADELTA optimizer    \n",
    "optimizer = optim.Adadelta(model.parameters(), lr=1)\n",
    "start_epoch = 0\n",
    "total_epochs = 11\n",
    "best_loss = float(\"inf\")\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "# Load from checkpoint if exists\n",
    "checkpoint = load_checkpoint()\n",
    "if checkpoint:\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    val_losses = checkpoint['val_losses']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print('Loaded checkpoint from epoch %d' % start_epoch)\n",
    "for epoch in range(start_epoch, total_epochs):\n",
    "    print('Start training for epoch %d' % start_epoch + 1)\n",
    "    # train for one epoch\n",
    "    train_loss = train(train_loader, model, optimizer, use_gpu)\n",
    "    train_losses.append(train_loss)\n",
    "    print('Training loss for epoch %d: %.3f' % (start_epoch + 1, train_loss))\n",
    "    \n",
    "    # evaluate on validation set\n",
    "    print('Start validation for epoch %d' % start_epoch + 1)\n",
    "    val_loss = validate(val_loader, model, use_gpu)\n",
    "    val_losses.append(val_loss)\n",
    "    print('Validation loss for epoch %d: %.3f' % (start_epoch + 1, val_loss))\n",
    "    \n",
    "    new_checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer' : optimizer.state_dict()\n",
    "    }\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        # save the best checkpoint\n",
    "        print('Saving the new best checkpoint')\n",
    "        save_checkpoint(new_checkpoint, 'best_checkpoint.pth.tar')\n",
    "    # save checkpoint\n",
    "    print('Saving the checkpoint for epoch %d' % start_epoch + 1)\n",
    "    save_checkpoint(new_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
